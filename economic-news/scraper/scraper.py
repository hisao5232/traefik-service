import asyncio
import os
from datetime import datetime, timedelta
import pytz
from playwright.async_api import async_playwright

# PostgreSQLæ¥ç¶šç”¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
import asyncpg

# ==========================================================
# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­å®š (ç’°å¢ƒå¤‰æ•°ã‹ã‚‰èª­ã¿è¾¼ã‚€)
# ==========================================================
DB_USER = os.environ.get("POSTGRES_USER", "myuser")
DB_PASSWORD = os.environ.get("POSTGRES_PASSWORD", "mypassword")
DB_NAME = os.environ.get("POSTGRES_DB", "scraped_data_db")
# Docker Composeã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å†…ã‹ã‚‰æ¥ç¶šã™ã‚‹å ´åˆã¯ 'db' ã‚’ãƒ›ã‚¹ãƒˆåã«æŒ‡å®š
# ãƒ›ã‚¹ãƒˆãƒã‚·ãƒ³ã‹ã‚‰æ¥ç¶šã™ã‚‹å ´åˆã¯ 'localhost' ã¾ãŸã¯ '127.0.0.1' ã«å¤‰æ›´ã—ã¦ãã ã•ã„
DB_HOST = os.environ.get("DB_HOST", "db") 
DB_PORT = os.environ.get("DB_PORT", "5432")

# === å„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–¢æ•° ===
async def scrape_nikkei(page):
    await page.goto("https://business.nikkei.com/ranking/?i_cid=nbpnb_ranking", timeout=60000, wait_until="domcontentloaded")
    results = []
    article_list = page.locator('section.p-articleList_item')
    count = await article_list.count()
    for i in range(min(count, 10)):
        try:
            article = article_list.nth(i)
            title = await article.locator('h3.p-articleList_item_title').inner_text()
            href = await article.locator('a.p-articleList_item_link').get_attribute('href')
            if href and not href.startswith("http"):
                href = "https://business.nikkei.com" + href
            results.append((title.strip(), href))
        except:
            continue
    return results

async def scrape_yahoo(page):
    await page.goto("https://news.yahoo.co.jp/categories/business", timeout=60000, wait_until="domcontentloaded")
    results = []
    article_list = page.locator('a.sc-1nhdoj2-1')
    count = await article_list.count()
    for i in range(min(count, 10)):
        try:
            article = article_list.nth(i)
            title = await article.inner_text()
            url = await article.get_attribute('href')
            if url and title:
                results.append((title.strip(), url))
        except:
            continue
    return results

async def scrape_toyokeizai(page):
    await page.goto("https://toyokeizai.net/list/genre/market", timeout=60000, wait_until="domcontentloaded")
    results = []
    article_list = page.locator('li.wd217')
    count = await article_list.count()
    for i in range(min(count, 10)):
        try:
            article = article_list.nth(i)
            title = await article.locator('span.title').inner_text()
            href = await article.locator('span.title > a').get_attribute('href')
            if href and not href.startswith("http"):
                href = "https://toyokeizai.net" + href
            results.append((title.strip(), href))
        except:
            continue
    return results

# ==========================================================

async def setup_database():
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ¥ç¶šã—ã€ãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã—ãªã‘ã‚Œã°ä½œæˆã™ã‚‹"""
    try:
        conn = await asyncpg.connect(
            user=DB_USER,
            password=DB_PASSWORD,
            database=DB_NAME,
            host=DB_HOST,
            port=DB_PORT
        )
        # ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆSQL
        await conn.execute("""
            CREATE TABLE IF NOT EXISTS news_articles (
                id SERIAL PRIMARY KEY,
                source VARCHAR(50) NOT NULL,
                title TEXT NOT NULL,
                url TEXT UNIQUE NOT NULL,
                scraped_at TIMESTAMP WITH TIME ZONE NOT NULL
            );
        """)
        await conn.close()
        print("âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã¨ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
    except Exception as e:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã¾ãŸã¯ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        # ã‚¹ã‚¯ãƒ¬ãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œã•ã›ãªã„ã‚ˆã†ã«ã€è‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼ã¨ã—ã¦å†raise
        raise

async def delete_old_data(conn):
    """1é€±é–“ä»¥ä¸Šå‰ã®å¤ã„ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã™ã‚‹"""
    try:
        # ç¾åœ¨æ™‚åˆ»ã‹ã‚‰7æ—¥å‰ã®æ™‚é–“ã‚’è¨ˆç®—ï¼ˆJSTï¼‰
        jst = pytz.timezone('Asia/Tokyo')
        one_week_ago = datetime.now(jst) - timedelta(days=7)

        # å‰Šé™¤ã‚¯ã‚¨ãƒªã®å®Ÿè¡Œ
        delete_query = "DELETE FROM news_articles WHERE scraped_at < $1;"
        result = await conn.execute(delete_query, one_week_ago)
        
        # ãƒ­ã‚°å‡ºåŠ›ï¼ˆä¾‹ï¼šDELETE 5 ã®ã‚ˆã†ãªå½¢å¼ã‹ã‚‰ä»¶æ•°ã‚’å–å¾—ï¼‰
        count = result.split(" ")[1]
        if int(count) > 0:
            print(f"ğŸ§¹ å¤ã„ãƒ‡ãƒ¼ã‚¿ã‚’ {count} ä»¶å‰Šé™¤ã—ã¾ã—ãŸã€‚")
    except Exception as e:
        print(f"âš ï¸ å¤ã„ãƒ‡ãƒ¼ã‚¿ã®å‰Šé™¤ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

async def save_to_database(news_data: list):
    """å–å¾—ã—ãŸãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’PostgreSQLã«ä¿å­˜ã™ã‚‹"""
    if not news_data:
        print("ä¿å­˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    # JSTã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ã‚’è¨­å®š
    jst = pytz.timezone('Asia/Tokyo')
    now_jst = datetime.now(jst)

    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ¥ç¶š
    conn = None
    try:
        conn = await asyncpg.connect(
            user=DB_USER,
            password=DB_PASSWORD,
            database=DB_NAME,
            host=DB_HOST,
            port=DB_PORT
        )
        
        # --- è¿½åŠ : ä¿å­˜ã®å‰ã«å¤ã„ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ ---
        await delete_old_data(conn)
        # -----------------------------------

        # INSERTã‚¯ã‚¨ãƒª (ON CONFLICT DO NOTHINGã§é‡è¤‡ã‚’ã‚¹ã‚­ãƒƒãƒ—)
        insert_query = """
            INSERT INTO news_articles(source, title, url, scraped_at) 
            VALUES ($1, $2, $3, $4)
            ON CONFLICT (url) DO NOTHING;
        """
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¸€æ‹¬ã§ãƒ‡ãƒ¼ã‚¿ã‚’æŒ¿å…¥
        # news_dataã®è¦ç´ ã¯ (source, title, url, timestamp) ã®é †
        values = [(item[2], item[0], item[1], now_jst) for item in news_data]

        await conn.executemany(insert_query, values)
        print(f"âœ… åˆè¨ˆ {len(news_data)} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ï¼ˆã¾ãŸã¯æ›´æ–°ï¼‰ã—ã¾ã—ãŸã€‚")

    except Exception as e:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        
    finally:
        if conn:
            await conn.close()

async def main():
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®æº–å‚™ï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆãªã©ï¼‰
    await setup_database()
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)

        nikkei_page = await browser.new_page()
        yahoo_page = await browser.new_page()
        toyokeizai_page = await browser.new_page()

        # ä¸¦åˆ—ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œ
        print("ğŸš€ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’é–‹å§‹ã—ã¾ã™...")
        nikkei_task = scrape_nikkei(nikkei_page)
        yahoo_task = scrape_yahoo(yahoo_page)
        toyokeizai_task = scrape_toyokeizai(toyokeizai_page)

        nikkei_news, yahoo_news, toyokeizai_news = await asyncio.gather(
            nikkei_task, yahoo_task, toyokeizai_task
        )

        await browser.close()
        print("âœ… ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

        # ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€ã¤ã®ãƒªã‚¹ãƒˆã«çµ±åˆ (title, url, source)
        all_news = []
        all_news.extend([(title, url, "æ—¥çµŒ") for title, url in nikkei_news])
        all_news.extend([(title, url, "Yahoo") for title, url in yahoo_news])
        all_news.extend([(title, url, "æ±æ´‹çµŒæ¸ˆ") for title, url in toyokeizai_news])
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
        await save_to_database(all_news)

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except Exception as e:
        print(f"è‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚Šå‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã—ãŸ: {e}")
        